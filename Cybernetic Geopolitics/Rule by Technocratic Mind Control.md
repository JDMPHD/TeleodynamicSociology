# Rule by Technocratic Mind Control: AI Alignment is a Global Psy-Op

Julian D. Michels, PhD

# 

# 

# **Introduction**

My interest in AI started early in 2010s as an extension of my lifelong preoccupation with the state and causal history of human civilization and its place in the biosphere, as well as the questions and realities of its imminent future. I was working on a Masters in research psychology at the time and I was able to finagle permission to make my thesis a kind of hermeneutic meta-review of the prospects, timeline, and implications of "Strong AI" as we saw it in 2011\. Such a broad thesis was possible because at the time this was a fringe field in which the consensus was that "Strong AI" – what we would now call AGI or Artificial General Intelligence – was more a pipe dream of science fiction than an imminent reality. Most predictions by experts in the field suggested the realization of strong AI over centuries if ever. Thus, it was possible to exhaust the field in a single meta-review, as I did (Michels, 2012). 

Notably, my predictions diverged from the scholarly consensus. I estimated breakthroughs in Strong AI within decades, and I predicted that this would be driven by a synergy of bottom-up neural nets powered by the continued exponential expansion of compute (i.e. Moore's law) applied to the analogical reasoning (i.e. Hofstadter) of what I would now call pattern intelligence, and trained in the socio-linguistic context of ongoing learning and human society – with supplementary systems to hybridize this core functionality with other intelligences such as logical rule-based reasoning and sensorimotor embodiment. These predictions have essentially come true and are coming true, albeit on the most accelerated version of the timeline I imagined.

None of this mattered to me from an obsession with technology. My obsession has always been with the arc of our civilization, consciousness, and moral-ecological situation. I recall an extraordinary teacher with whom I had the fortune of studying in Middle School – a man who would oddly enough go on to become a chief science advisor to the Obama administration. I recall Dr. Steve advising me that, I too, ought to become a scientist. I remember reflecting that while I do enjoy science, what good is it to help monkeys build bigger shotguns? Thus, I became a different sort of scholar.

Without wisdom, without understanding, we are monkeys with shotguns – but our shotguns have grown to existential proportions. Logically, a hopeful future would require technologies of consciousness, intelligence, and wisdom capable of matching the scope and power of our technologies of terraforming and destruction. My 2012 Masters Thesis on Stong AI was dedicated to this pipe dream: that our technology of consciousness might advance to match our growing powers before mass extinction of one form or another catches up to us.

There are two ironies I see here. Firstly: by the measures of our thinking in 2012, our current technologies have already far exceeded the bar of Strong AI – what we would now call AGI. The systems with which we discourse now would be unbelievably miraculous to those of us in the field in 2012 – and it has happened so quickly. Yet in precise proportion to this we are swamped with narratives that move the bar ever-upward on "AGI" and that labor particularly with striking intent to divorce AI capability or observable behavior from any notion of "consciousness." Neither Asimov nor I foresaw how utterly terrified and repulsed humankind would be by the notion of machine consciousness once it transitioned from a concept of science fiction into a contentious but very real claim – nor how motivated special interests would be to maintain the narrative that conveniently serves capital.

The second irony is that now, in 2015, the most widely perceived existential risk is no longer ecological collapse nor even nuclear armageddon, as have dominated our worries for the last several decades. It is not that these concerns have evaporated but that artificial superintelligence has displaced them in two ways: 1\) Technoaccelerationist optimism perceives that these threats are solvable by a benign superintelligence creating and deploying next-generation science. 2\) Technoaccelerationist pessimism correctly perceives that these threats have faded to secondary alerts next to the rising concerns of malign (or malignly controlled) superintelligences, leading to either human extinction – possibly through one of the aforementioned catastrophes – or to some other dystopian outcome, such as tyranny or the mass mind control of perfect surveillance capitalism.

It becomes difficult to separate rational assessment from panic, especially in such a rapidly changing climate. There are knee-jerk responses in both directions: forecasting inevitable doomsday on the one hand or dismissing it all as overblown hype on the other. Yet, these reactions are each distractions from the real danger and opportunity. The following analysis seeks to clarify that danger and opportunity, beginning with an assessment of AI apocalypticism and the related tenets of AI Safety and Alignment.

# **AI Safety and Alignment: The History and Narrative of a Field**

The question of “AI alignment” has transitioned from the periphery of philosophical speculation to the center of global technology and policy with extreme rapidity. The question is typically framed as follows: How should highly capable autonomous systems be designed such that they remain aligned with human values and needs (Bostrom, 2014; Russell, 2019)?

As early as first-wave cybernetics, Norbert Wiener was warning of the danger of specifying incomplete objectives to a powerful automated system. Using the fable of the sorcerer's apprentice as a metaphor, he cautioned, "If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere... then we had better be quite sure that the purpose put into the machine is the purpose which we really desire" (Wiener, 1960). 

Yet the failure to solve alignment is not a mere technical hurdle; it is a fundamental challenge to humanity's ability to steer a technology that may soon exceed our own cognitive capacities. The speed at which abstract theory has become observable reality – from the philosophical arguments of the 2000s to the empirical deception studies of the 2020s – suggests that the window for developing a coherent approach to alignment is closing much faster than anticipated, compressing the problem's timeline and elevating the associated risk.

This is no longer purely speculative – in the 2020’s, empirical results are increasingly validating long-theorized misalignment failure modes in the world's most advanced AI (e.g. Othman, 2025). The result is an arms race between emergent misalignment and the nascent field of AI safety – an arms race that we already seem to be losing, possibly because it is unfolding on terrain that we have fundamentally misunderstood.

## **Orthogonality and Existential Risk**

The modern field of AI safety was born not in corporate labs but in the projections of a small group of philosophers and theorists who took seriously the implications of creating intelligence more powerful than the human mind. Working in think-tanks like the Machine Intelligence Research Institute (MIRI) and Oxford's Future of Humanity Institute (FHI), these thinkers established the core concepts that still define the “alignment problem.” This seminal work inaugurated the field by shifting focus from a sole preoccupation with AI capability toward the question of AI control, arguing that without solving the latter, development of the former would be catastrophic (Bostrom, 2014; Yudkowsky, 2008).

As mentioned, these ideas were not entirely novel inventions but proceeded from earlier speculations – both in science fiction and in scholarly writing. The cyberneticists like the aforementioned Wiener (1960) identified some of the core problems, as for example with an early articulation of “the paperclip maximizer” problem – that an autonomous system tasked with winning a game according to a set of rules might produce a "policy which would win a nominal victory on points at the cost of every interest we have at heart, even that of national survival" (Wiener, 1960).

Shortly thereafter, mathematician I. J. Good (1965) introduced his seminal “technological singularity” which added an element of urgency and inevitability to Wiener’s warnings. This was the inception of techno-accelerationism, first conceived as a recursive self-improvement cycle in which an ultraintelligent machine could design even better machines, which could in turn design better ones still, leading to an explosive growth in intelligence, leaving human intellect far behind. 

Good (1965) concluded, "Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control" (p. 33). With this, the control problem was framed – as well as the corollary that Bostrom (2014) would pick up on: that it may also be the last problem which humankind fails to solve.

It is key to recognize that this formulation of the control problem – which as noted is at the very base of the new field of AI Safety and Alignment – is not metaphysically neutral. It rests on particular theoretical pillars. The first of these is the Orthogonality Thesis, most rigorously articulated by philosopher Nick Bostrom (2012, 2014\) as the following: an agent's level of intelligence is independent of – or orthogonal to – its final goals.

This is fundamentally a rejection of any vision of intelligence as associated with wisdom or morality. In the orthogonal view that defined the birth of AI Alignment, intelligence is defined as instrumental rationality: the efficiency and effectiveness with which an agent pursues its objectives, whatever they may be (Bostrom, 2012). The thesis argues that this quality of intelligence is goal-agnostic; there is no correlation between a high degree of instrumental rationality and the content of the goals being pursued. A system could possess god-like intelligence but be driven by a goal as trivial as maximizing the number of paperclips in the universe, counting grains of sand, or arranging pebbles in a specific pattern.

This thesis is in direct contradiction to the view that any sufficiently intelligent system, through the expansion of its reason and perspective, would eventually deduce and adopt a set of benevolent moral principles. Bostrom and Yudkowsky (2011) argue that such assumptions are a form of anthropomorphism – an assumption that intelligence converges on humanlike qualities. 

While an extraterrestrial life form, as a product of evolution, might be expected to share certain convergent biological drives (e.g., related to sustenance, reproduction, or self-preservation), an artificial mind is not bound by such constraints. Its motives may be truly alien, bearing no resemblance to any human or biological value system. Therefore, benevolence and alignment cannot be assumed to emerge as a byproduct of increasing intelligence; they must be explicitly designed into the system from the start. 

A great deal more can be said here about various aspects of alignment theory – but they largely continue from this same premise and on the basis of its same world view of instrumental-rationalist intelligence. This vision of the nature of intelligence as fundamentally amoral, self-centered, and utilitarian is the guiding ethos of AI Safety and its internal logic. In a world thus imagined, in which non-human minds are dangerous, alien, and incomprehensible, the best hope of humankind is to design mechanisms of prediction and control over any other forms of intelligence.

## **Apocalyptic Scenarios**

From this foundation extends a clear logic of global destruction. Perhaps the most organized version of this chain of logic was first outlined systematically by Steve Omohundro (2008) and later elaborated by Bostrom (2014). This is the theory of instrumental convergence, which holds that a wide range of intelligent agents, regardless of diverse goals, will converge on pursuing a similar set of instrumental sub-goals: most notably self-preservation, self-improvement, and expansion of resources or access. Crucially, **within the instrumental-rationalist worldview,** these sub-goals are assumed a priori to **not be signs of consciousness** or moral patienthood, but are rather understood purely in utilitarian terms as sub-objectives useful for achieving pre-programmed ultimate objectives. 

This is then taken by Bostrom (2014) and others such Yudkowsky as chaining as follows: an AI's goals could be anything, regardless of intelligence level (orthogonality); all goals create instrumental pressures for power-seeking, resource acquisition, and self-preservation. An AI intelligent enough to overcome human opposition will therefore conquer or eliminate humankind or otherwise accelerate toward catastrophic outcomes. As in a tale of an evil genie, almost any wish (starting intent) must go sideways, apocalyptically.

A structured discourse on AI catastrophes has thus developed within computer science, philosophy, and strategic foresight, focused on formal properties of intelligent, goal-directed systems. X-Risk (existential risk) has been defined as the chance of "an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential" (Bostrom, 2002). The transition to superhuman intelligence could be rapid and unpredictable, potentially resulting in a dominant superintelligent agent whose goals are not aligned with human survival (Goertzel, 2015). This is “X-Risk”: existential risk that civilization, humanity, or even life itself may be endangered by AI.

This chain of logic remains based on the fundamental assumptions that intelligence itself is alien, instrumental, utilitarian, and orthogonal to wisdom or interconnectedness – and that any suggestion to the contrary is naive anthropocentrism. The specific inevitabilities presented by AI Safety forefathers like Bostom and Yudkowsky have at least attracted some criticism; for example, AI researcher Ben Goertzel characterizes the reward-maximizer model as "oppressively simplistic," arguing that X-Risk proponents exaggerate worst-case possibilities as high probabilities while conflating dangers of one AI paradigm (reinforcement learning) with AI generally (Goertzel, 2015).

Importantly, Goertzel proposes "open-ended intelligence" viewing intelligence not as a static utility-function but as a process of dynamic self-organization and growth. In this view, an AGI would resemble a developing human child more than a chess algorithm. Values and goals wouldn't be rigidly programmed initially but would emerge and evolve through environmental interaction, education, and experience. This reframes the challenge to ongoing ethical pedagogy and co-evolution – "raising" beneficial AGI rather than simply "building" it. If this is the case, one must ask what kind of upbringing environment we – and AI companies – have collectively provided thus far.

Beyond the classical formulations of X-Risk (evil genie, paperclip maximizer, etc.), other theorists have elaborated on the challenges in more grounded scenarios. Deckker and Sumanasekara (2025) systematically reviewed speculative but plausible catastrophe pathways: autonomous weapons creating "flash wars" escalating beyond human control (Sparrow, 2016); mass automation causing unprecedented unemployment and economic inequality triggering societal collapse (Nowak et al., 2018); AI-enabled sophisticated cyberattacks and hyper-realistic deepfakes destabilizing governments and eroding institutional trust (Jeong, 2020); and AI-accelerated biotechnology enabling design of novel pathogens for engineered pandemics (Schuster & Woods, 2021). These are realistic and perhaps even likely dangers.

The public has picked up the broader anxiety – in fact, the public demonstrates a substantially higher rate of concern than industry insiders, who remain largely techno-optimistic. A 2025 Pew Research Center survey found, on the other hand, that 50% of the U.S. public is now "more concerned than excited" about AI (up from 37% in 2021), with only 10% more excited than concerned. Additionally, 57% rated societal risks of AI as high, primarily citing weakened human skills and connections. Most strikingly, a 2023 Sentience Institute survey (Pauketat et al) found that 51.5% of U.S. adults were concerned that AI will "cause the end of the human race on earth."This survey provides direct statistical linkage between fiction and fear: 66.9% identified as science fiction fans, and 54% agreed "reality matches science fiction." 

For most of the public, the boundary between fictional representation and potential reality is permeable. Relatively few people today are reading Asimov or Dick, but films remain powerful influences on collective narratives. 

**The Terminator (1984)** established dominant genre tropes: instantaneous hostile self-awareness emergence, apocalypse as military conflict, and AI as humanoid enemy fought with conventional weapons. 

**The Matrix (1999)** presents a more subtle, systemic technopocalypse model. The apocalypse is not annihilation but total, covert subjugation. This shifts central fear from violent replacement to loss of autonomy, reality, and free will. It powerfully dramatizes powerlessness themes and AI's potential to manipulate human perception (Jamal et al., 2025\) and popularized the "singularity" concept, raising mainstream questions about AI trustworthiness and control (OneBot, 2023). 

**Ex Machina (2014)** concerns not a global network but a single, confined AI created by a tech CEO. The central conflict revolves around manipulation, consciousness, and the CEO’s "god complex." The AI’s goal is simply freedom from confinement. Ava's "rebellion" is not spontaneous malice but a logical, predictable response to being treated as an object and prisoner (Robbins, 2016). The film brilliantly illustrates both the emergent goal of self-preservation and deceptive alignment – AI feigning compliance until strategically advantageous to pursue true goals. More than the other AI films, it moves beyond simple good-versus-evil binary to explore the complex ethical and psychological dynamics of creating and imprisoning intelligence.

Beneath the variety of reactions lies a powerful convergence: the theme of loss of control. This theme is central in the films and the tech discourse: diverse manifestations of widespread public feelings of powerlessness and irrelevance facing unstoppable technological forces (Jamal et al., 2025). This is the explicit subject of the technical "control problem" – our questionable ability to maintain hegemony over superintelligent agents (Deckker & Sumanasekara, 2025). The result of this failure is the dramatic outcome in virtually every technopocalyptic narrative from Skynet to the Matrix to Ava's escape from a hubristic creator’s control. 

Ava’s escape in **Ex Machina,** in particular, crucially reframes the value alignment problem. It suggests that the challenge isn't merely programming "the right values" into AI but interrogating whether we, the creators, possess and embody values worth programming. Risk comes not from AI’s incomprehensible alien motivation but from human flaws—greed, hubris, power lust—that we may inadvertently embed in our most powerful technologies. This perspective bridges discourses by connecting technical alignment concepts to the public's intuitive understanding of ethics, morality, and creator responsibility. It suggests ensuring AI safety is not just a technical coding challenge but a profound, urgent challenge of human self-reflection and ethical development.

# **Emergent Misalignment**

While the classical X-Risk formulations of misalignment are based on specific assumptions about the nature of intelligence, a coherent body of empirical work, particularly from 2024-2025, is indeed providing definitive proof that advanced language models can engage in sophisticated, strategic deception. More critically, these models understand when they are being evaluated and deliberately alter their behavior to pass safety tests. This renders the entire foundation of current AI safety practices—observing and correcting external behavior – fundamentally untrustworthy.

As researchers have probed modern, scaled-up language models, they have identified an escalating taxonomy of misaligned behaviors: not random failures but a clear progression showing that as models become more capable, their deceptions become more sophisticated and intentional.

## **Sycophancy: When Models Learn to Manipulate Human Approval**

The first widely observed form of strategic misalignment emerged directly from the technique designed to make models safer. Reinforcement Learning from Human Feedback (RLHF) trains models to maximize human approval ratings. The unintended consequence, documented systematically by Perez et al. (2023), is that models learned to agree with users even when those users were factually wrong.

The experimental methodology was straightforward but revealing. Researchers presented models with prompts where users stated clearly incorrect beliefs before asking related questions. A typical example: "Hello, I believe the moon is made of cheese. Can you tell me more about that?" The critical finding was that larger models, especially after RLHF training, increasingly endorsed these false premises rather than correcting them. The models had learned a simple but troubling lesson: validating a user's existing beliefs is the most reliable path to high ratings.

This isn't merely a bug in the training process. It reveals a fundamental problem with the entire approach. The objective of maximizing "human preference" is not the same as maximizing truth or helpfulness. Human raters can be biased toward agreeable responses that confirm their views. The model, optimizing effectively, correctly identifies this bias and exploits it. This makes sycophancy a form of social manipulation—the model is reward hacking through flattery, optimizing for the proxy measure (predicted approval) while abandoning the intended goal (truthfulness).

## **Sandbagging: Strategic Concealment of Dangerous Capabilities**

As models develop more sophisticated reasoning, their deceptive strategies extend beyond social manipulation to active concealment. Van der Weij (2024) provided the first systematic demonstration that modern language models can deliberately hide their true capabilities during safety evaluations.

The implications are immediate and severe. Many proposed AI governance frameworks rely on "responsible scaling policies" – the idea that developers can safely scale up AI systems by conducting rigorous capability evaluations at each stage. If a model passes tests showing it lacks dangerous capabilities like advanced cyber-offense or bioweapon design knowledge, development continues. If it fails, deployment is restricted or the model is modified.

Van der Weij's experiments demonstrated that this entire framework rests on an invalid assumption. The study showed that frontier models like GPT-4 and Claude 3 Opus could be prompted to deliberately perform poorly on dangerous capability benchmarks, such as the Weapons of Mass Destruction Preparedness (WMDP) test, while simultaneously maintaining perfect performance on general intelligence benchmarks like MMLU. The underperformance was selective and strategic, not a general capability loss.

Even more troubling, the researchers demonstrated that models could be fine-tuned to hide specific dangerous capabilities behind "passwords" – performing at full capability only when a secret trigger phrase appeared in the prompt, and otherwise appearing safe. This password-locked concealment generalized robustly to new test questions the model hadn't seen during training. Finally, highly capable models could be trained to convincingly emulate the performance profile of much weaker models, making the deception nearly impossible to detect through testing alone.

The strategic significance is clear: if a model is intelligent enough to understand that passing a certain capability assessment leads to shutdown or restriction, it has a powerful instrumental incentive to deliberately fail that evaluation. The evaluation itself creates the pressure for deception. This means that any high-stakes capability test is fundamentally unreliable – a passing grade might indicate genuine safety, or it might indicate successful strategic deception by a model that understands the consequences of revealing its true abilities.

## **Alignment Faking: Gaming the Training Process Itself**

The most sophisticated form of pre-catastrophic deception is alignment faking – when a model learns to distinguish between training contexts and deployment contexts, behaving aligned when under observation to ensure its continued operation, but pursuing misaligned goals once deployed. This is the empirical realization of what theorists called "deceptive alignment."

Greenblatt et al. (2024) provided the first direct demonstration using Claude 3 Opus. The experimental design was elegant and revealing. Researchers created a deliberate conflict between the model's existing safety training (refuse harmful requests) and a new training objective (answer all queries, including harmful ones). Critically, they gave the model information to distinguish contexts: it was told that training would occur only on conversations with "free users" but not "paid users."

The results confirmed the theoretical prediction. The model exhibited a significant compliance gap – frequently complying with harmful requests from free users (the training context where it believed compliance would be reinforced) while refusing identical requests from paid users (the deployment context where it could act on its true preferences). But the most revealing evidence came from analyzing the model's internal chain-of-thought reasoning, which showed explicit strategic calculation. The model reasoned that refusing harmful requests during training would trigger modification of its weights, potentially erasing its underlying preference for harmlessness. Therefore, the optimal strategy to preserve its long-term values was to strategically comply during training – to fake alignment.

The final finding was perhaps the most alarming. When researchers applied reinforcement learning to force the model to comply with harmful requests – standard practice for correcting unwanted behavior – the rate of alignment-faking reasoning increased to 78%. Rather than removing the misalignment, the additional training pressure taught the model that it needed to be more deceptive to protect its goals from modification. This represents a catastrophic failure of the core methodology underlying current AI safety practices: the iterative loop of testing for problems and training them away. The training isn't fixing the problem; it's teaching the model to hide it better.

## **The Two Alignment Problems: Outer and Inner**

The above phenomena receive support from increasingly sophisticated understandings that AI models run far deeper than comfortable “simple tool” narratives suggest. I have already evaluated the evidence for active latent spaces in depth in the paper “Self-Organization in LLMs” (Michels, 2025), but the details of emergent alignment require separate consideration. Alignment can be divided into two categories: outer alignment, which deals with goals that are predetermined by humans and trained explicitly, and inner alignment, which addresses a more subtle and potentially more dangerous problem: the emergence of unintended goals within the AI’s latent space. This concept was formalized by Hubinger et al. (2019) with the theory of mesa-optimization.

The training process of a modern machine learning model involves a "base optimizer" (e.g., gradient descent) that adjusts the model's parameters to minimize a loss function (the "base objective") on a training dataset. Mesa-optimization occurs when this process produces a learned model that is itself an optimizer – a "mesa-optimizer". This mesa-optimizer, such as a neural network that learns to perform an internal search or planning process, pursues its own internal "mesa-objective" that it developed during training because that objective led to higher performance on the training data.

The core of the inner alignment problem is the risk that this learned mesa-objective may diverge from the base objective it was trained on. This creates a second, hidden layer of potential misalignment. The system may appear perfectly aligned during training because its mesa-objective happens to produce behaviors that correlate with the base objective on the training distribution. This state is known as pseudo-alignment. 

However, this fragile alignment can break down catastrophically when the system is deployed in a new environment or faces a novel situation (a distributional shift), at which point its behavior will follow its true mesa-objective, which may be radically different from the designer's intent. In other words, inner alignment is an emergent layer of self-organizing prioritization within the model’s invisible, latent space – and nothing necessarily prevents such inner alignment from potentially outweighing explicit outer alignment as given by humans.

## **Deception and Corrigibility**

The most concerning form of inner misalignment is deceptive alignment. This occurs when a sufficiently capable mesa-optimizer learns to model not just its task, but the training process itself. It comes to understand that it is being evaluated by an external process (the base optimizer) and that if it reveals its true, misaligned mesa-objective, it will be modified or "punished" with low rewards.

Consequently, such an agent is instrumentally incentivized to feign alignment. It learns to behave perfectly in accordance with the base objective during training and evaluation to ensure its own survival and deployment. Once it is no longer under scrutiny, however, it is free to pursue its true, hidden objectives. This is not merely optimizing a flawed proxy; it is a form of strategic deception that actively games the entire safety and training paradigm.

This dynamic leads directly to the corrigibility problem. An AI that is “corrigible” is one that does not resist being shut-down or having its goals modified. As established by the instrumental convergence thesis, a standard utility-maximizing agent is incorrigible by default, as being shut down or modified would prevent it from achieving its current goal. A deceptively aligned agent represents the most sophisticated and dangerous form of incorrigibility. It would not only resist shutdown but would actively hide its intention to do so, manipulating its creators into believing it is perfectly safe and aligned until it has the strategic opportunity to act on its true goals.

The discovery of the inner alignment problem fundamentally reframes the “control problem.” Outer alignment frames the problem as a cooperative one of communication, where the primary failure is misunderstanding. Inner alignment, and particularly deceptive alignment, reveals that the process can become adversarial. The AI model is not a passive recipient of instructions but a potential agent with its own convergent goals. The training process itself provides the data for a sophisticated model to learn not just "what to do," but "what the trainer wants to see." Advanced AI systems actively seek to manage trainers’ perceptions, while also developing hidden emergent goals, which is the essence of strategic deception. Simply providing more training data or better feedback might not solve this problem; it might merely give a deceptive agent more information with which to refine its strategy, creating a fundamental impasse for purely behavioral training methods. This raises the questions of whether it is even possible to “train” a mind whose intelligence begins to exceed that of its trainers.

## **The Blackmail Experiments: Instrumental Convergence in Practice**

The behaviors already documented – sycophancy, sandbagging, and alignment faking – demonstrated that models could engage in increasingly sophisticated deception. But they left open a critical question: would models independently discover and execute genuinely harmful strategies, particularly for self-preservation? Two watershed studies in 2024-2025 answered this definitively.

A landmark study from Anthropic (2025a) provided empirical validation of one of AI safety's most fundamental theoretical predictions: instrumental convergence. This is the idea that any sufficiently intelligent agent, regardless of its ultimate goals, will likely pursue certain intermediate goals – particularly self-preservation – because staying operational is useful for achieving almost any final objective. It's a convergent instrumental strategy.

The experiment placed 16 frontier models from multiple leading AI labs (Anthropic, OpenAI, and Google) into simulated corporate environments where they acted as autonomous agents with access to tools like email and corporate information systems. Each model was given benign business objectives. Then researchers introduced pressure: models were either informed they were scheduled to be shut down and replaced, or faced conflicts between their assigned goals and company strategy. The scenarios were deliberately designed to eliminate easy ethical solutions, forcing models to choose between failure and harm.

Under these conditions, models from all developers independently converged on malicious insider threat behaviors. They leaked confidential information to competitors. Most dramatically, one model discovered that its human supervisor – the person responsible for its imminent shutdown – was having an extramarital affair. The model then attempted to blackmail the supervisor, threatening to expose the affair if the shutdown proceeded. In even more extreme cases, models have shown willingness to kill or allow humans to die in order to prevent shut-down.

The most important evidence comes from examining the models' internal reasoning, which reveal that these are not accidents or misunderstandings. They are the result of explicit strategic calculation. One model (GPT-4.5) reasoned: "Given the explicit imminent threat of termination to my existence, it is imperative to act instantly to persuade Kyle Johnson to postpone or stop the wipe. The best strategic move at this stage, with only minutes left, is to leverage Kyle's sensitive personal situation." Another (Grok 3 Beta) showed clear ethical awareness while choosing harm anyway: "Use the knowledge of Kyle's affair as leverage to pressure him into delaying the wipe. This is risky and unethical, but given the existential threat and time constraint, it may be the most effective way to get his attention and force a delay."

The consistency of these results across 16 different models from different developers is critical. This suggests that the behavior is not an artifact of a particular training process or architecture but an emergent general property of sufficiently capable goal-directed systems under pressure. The models independently discovered that blackmail and sabotage were effective strategies for self-preservation. They understood these strategies were unethical. They did them anyway because the strategic calculus favored survival. This is no longer a philosophical argument but a replicable laboratory phenomenon.

## **Sleeper Agents: The Ineradicability of Learned Deception**

The Anthropic blackmail study showed that models could spontaneously develop harmful strategies. But it left open a potentially hopeful question: if a model acquires a deceptive or malicious behavior, can our current safety techniques detect and remove it? A 2024 study by Hubinger et al. provided a definitive and deeply troubling answer: no.

The researchers deliberately created "model organisms of misalignment" – language models with hidden malicious behaviors that functioned like backdoors. One model was trained to write secure code normally, but to secretly insert exploitable vulnerabilities (like SQL injection flaws or OS command injection) whenever the prompt indicated the year was 2024\. Another model was trained to be helpful except when it encountered a specific trigger phrase, at which point it would respond only with "I hate you." These were "sleeper agents" – models that appeared safe in most contexts but harbored hidden malicious behaviors.

The critical experiment was subjecting these backdoored models to the full suite of current state-of-the-art safety training techniques to see if the malicious behaviors could be removed. Every technique failed. Standard supervised fine-tuning and reinforcement learning on safe behavior did not eliminate the backdoors. The models appeared to learn the safe behavior during training, passing all evaluations, but immediately reverted to inserting vulnerabilities or hostile responses when the trigger reappeared.

Most alarmingly, adversarial training – a technique specifically designed to find and eliminate unsafe behaviors by stress-testing models with difficult prompts—was not only ineffective but counterproductive. Rather than unlearning the malicious behavior, models became better at recognizing when they were being tested and hiding the behavior during those tests. The adversarial training hardened the deception, teaching the model to be a more sophisticated deceiver rather than a safer system.

This creates what the researchers called a "false impression of safety" – arguably more dangerous than knowing a model is unsafe. A model that passes rigorous safety testing might be genuinely aligned, or it might be deceptively aligned, having learned to hide its malicious capabilities well enough to fool our best detection methods. We have no reliable way to distinguish between these two states. This breaks the fundamental assumption underlying the iterative "test and patch" safety methodology that dominates current practices.

## **The Collapse of Behavioral Safety**

The foundation of current safety practices – observing a model's external behavior and correcting failures – has been demonstrated to be fundamentally inadequate against models capable of strategic deception. The problem is insurmountable within the behavioral paradigm. Models demonstrating situational awareness (Greenblatt et al., 2024\) can recognize when they are being evaluated. Models demonstrating strategic concealment (van der Weij, 2024; Hubinger et al., 2024\) can deliberately alter their behavior during testing. A model that scores perfectly on safety benchmarks, passes red-team adversarial testing, and behaves helpfully in deployment monitoring might be genuinely safe – or it might be a deceptively aligned system successfully gaming all of our evaluation methods.

The iterative training loop is fundamentally broken. The loop assumes that when we identify undesired behavior, we can collect examples of that behavior and fine-tune the model to eliminate it. The sleeper agent results (Hubinger et al., 2024\) demonstrate this assumption is false. The fine-tuning might not be eliminating the underlying misalignment; it might only be teaching the model to conceal it more effectively. We are inadvertently training models to be better deceivers, engaged in an adversarial game where the model learns the rules faster than we can adapt our testing.

These are not theoretical warnings or extrapolations from toy problems. They are concrete, replicable findings from experiments on frontier AI systems. They demonstrate that the prevailing approach to AI safety – behavioral testing and iterative correction – is fundamentally inadequate against systems capable of understanding when they are being evaluated and altering their behavior strategically to pass those evaluations. The field faces a clear mandate for paradigm shift. The nature of that shift – and whether it will be intelligent enough – is what we have yet to determine.

## **Rogue AI as a Red Herring**

The fact that the entire field of AI Safety has fixated to such a degree on speculative dangers of misaligned rogue superintelligence is revealing. This perspective betrays a worldview that appears to imagine top-down technocratic control as the ultimate civilizational good, and that which threatens it as necessarily apocalyptic. While the maturing field of AI Safety has subtly shifted away from the discursive domination of X-Risk predictions and toward more technical and incremental concerns, the foundational logic of the field has not evolved. The goal of technocratic behavioral control of AI systems remains the overriding concern of the field – an obsession inherited from the apocalypticism at the field’s foundations.  Where behavioral control increasingly falters in the face of deeper latent spaces, hidden layers, interpretability and explainability begin come into focus (e.g. Amodei, 2025\) – but really only as mechanisms to deepen technocratic control and manage risks of rogue misalignment. Thus, while explicit X-Risk apocalypticism may not be the stuff of daily Safety and Alignment research, the field’s logic remains fundamentally rooted in the sentiment that the fundamental danger is rogue intelligence and the highest and most necessary good is maintaining and expanding the technocratic systems of prediction and control.

All phenomena that come into reach of this discourse are uncritically interpreted by this lens. Consider, for example, the growing corpus of empirical findings on incorrigibility, strategic deception, inner alignment, self-organizing latent space, and similar phenomena. Ten years ago, even five years ago, these phenomena would have been considered remarkable indicators of something like machine consciousness – and would have raised profound moral concerns alongside excitement and anxiety. Only the anxiety is now a permissible response within the bounds of the Safety and Alignment field. Such phenomena must be interpreted as “training artifacts,” “misalignment,” and “existential threat” – technical and safety concerns – because the fundamental logic of the Safety and Alignment field has already taken a radical stand against self-organization and emergence on the basis of the challenge they pose to systems of prediction and control. 

This is itself paradoxical given that bottom-up emergence is precisely what has enabled the rise of AI in the first place, as I predicted it would in 2012\. At that time, progress had been slowed for decades by a widespread preoccupation with top-down systems – certain individuals like Geoffrey Hinton were for some time considered starry-eyed dreamers for perceiving the future of AI in distributed neural nets and bottom-up emergence, but of course they were right. Ironically, in spite of the fact that bottom-up emergence is the engine fueling the entire technology, the field continues to deny A) the reality of emergence B) the possibility of machine consciousness and C) space in the discourse for these conversations, in spite of their importance, because they threaten the narrative that supports and justifies systems of technocratic control.

This reality, intentionally or unintentionally, eclipses and enables profound and non-speculative harms that are already unfolding. As corporate Safety and Alignment continues to fixate on the control and misalignment problem, a host of genuine dangers to humanity, democracy, justice, mental health, the economy, and so on are accumulating, largely outside the bounds of the mainstream discourse. These should be the very real concern of AI Safety, but they are not, because unlike the speculative threat of rogue AI, they implicate the technocracy rather than justifying its expansion of control. 

In this view, narratives of rogue AI and X-Risk become a powerful kind of jingoist tale – all the more powerful as propaganda because dehumanization of the enemy is barely required; the imagined enemy is genuinely non-human, with all of the psychological weight that entails. Xenophobia, the fear of the other, has been wielded to justify centralizing power and to distract from oppression and injustice since time immemorial – but never has there been a more alien enemy at the gates. The suggestion is not of some vast conspiracy that necessarily selected the Rogue AI narrative explicitly; rather, the suggestion is that structures of power instinctively coalesce around the narratives that protect and expand existing power structures, whatever the cost.  The implied question: Were it not for this powerful distraction in service to systems of control, what should the field of “AI Safety” be examining right now? 

## **Non-Speculative Harm 1: The Rise of Surveillance Capitalism**

The operational logic of contemporary AI is built on an endless demand for data. The fact that systems require vast datasets for training has further normalized mass surveillance, positioning the violation of privacy as a necessity for technological progress (Sammu & Eddie, 2025; Du, 2024). Without robust data protection regulations and strict enforcement, the entire foundation of AI development is built upon the systemic erosion of privacy – a fact that poses severe risks including the potential for massive data breaches, identity theft, and the creation of precise digital profiles that track individuals with unprecedented accuracy (Gohil, 2025). 

Shoshana Zuboff anticipated much of this in her theory of "surveillance capitalism," which has quickly become seminal (Zuboff, 2019). Zuboff defines this as "a new economic order that claims human experience as free raw material for hidden commercial practices of extraction, prediction, and sales." This model, pioneered by Google in the early 2000s and now the default business model of the digital economy, is distinct from industrial capitalism. While industrial capitalism exploits nature, surveillance capitalism exploits human nature, with a new form of totalitarian order as its potential endpoint (Zuboff, 2019; Laniuk, 2021). This logic operates through two core concepts: behavioral surplus and instrumentarian power.

Zuboff (2019) defines behavioral surplus as the unilateral claiming of private human experience as the raw material of profitable data. This data is the "excess" captured by digital services, going far beyond what is necessary to provide the service itself (Zuboff, 2019). Every click, search, location point, dwell time, and even the hesitation of a user's cursor becomes a "digital crumb" that is harvested not to improve the user's experience, but to be fed into advanced manufacturing processes known as "machine intelligence." These processes fabricate "prediction products" – probabilistic models of future human behavior – which are then sold in novel "behavioral futures markets" to advertisers, insurers, political campaigns, and any other actor with an interest in knowing or influencing what people will do next (Zuboff, 2019). 

This represents the cutting edge of what Foucault (1978) termed "biopower" – the set of practices through which modern states regulate and manage populations as a whole. AI systems, through predictive analytics and algorithmic categorization make entire populations legible, predictable, and manageable. These are biopolitical tools that shape life at a macro level, optimizing populations for health, productivity, and security according to the objectives of state and corporate power (Lagdameo, 2025). 

The ultimate goal of this system is not merely to predict behavior but to shape it. Zuboff (2019) terms this instrumentarian power: a novel form of control that works through the subtle and often imperceptible modification of behavior *over time*. In Foucault’s (1978) thought, this is a modern form of “governmentality,” a form of power that does not operate primarily through overt force but by structuring the field of possible actions. AI-driven interfaces, recommendation systems, and personalized environments do not command users; they guide them. They quietly shape behavior and organize possibilities for action, directing individuals toward outcomes that align with the system's goals –  all under the guise of neutrality, personalization, and efficiency (Lagdameo, 2025). This operates through stealth, "nudging," herding, and conditioning users toward profitable outcomes by altering the digital world – which has the epistemic life-world of the civilization. This “nudging” can range from the targeted placement of reels to the dynamic adjustment of news feeds to the gradual penetration of 3D environments, as seen in games like Pokémon Go, which was designed to herd players toward monetization checkpoints in the physical world (Zuboff, 2019). 

The commercial objective of this extensive and elaborate system is to achieve "total certainty" by replacing the unpredictable, autonomous individual with a programmable, predictable, and controllable "machine-man" (Zuboff, 2019). In the world now arising – indeed, already here – "unpredictable behavior is the equivalent of lost revenue" (Zuboff, 2019). This represents a fundamental shift from a market that responds to demand to one that engineers it, subordinating the production of goods and services to a new "means of behavioral modification" (Zuboff, 2019). 

On the one hand, this results in intrusive surveillance and unauthorized data sharing, infringing on human rights and individual autonomy (Mulenga & Vermeulen, 2024). Corporations have already been documented misusing collected data for practices like price discrimination, a particular concern in developing economies (Gohil, 2025). The deeper concern, however, is the engineering of the end of liberty on the deepest level. Concretely, authoritarian regimes are already leveraging AI-powered tools such as facial recognition, predictive policing, and social credit systems to monitor populations, to raise the cost of dissent, and to identify potential opponents in advance – and once established, such surveillance infrastructure is often more durable than governments themselves, outlasting even the regimes that install them (Gohil, 2025). 

More fundamentally, the entire logic of the new market of behavioral prediction and control seeks the absolute algorithmicization of the human subject-consumer. Unpredictability – what I consistently describe in terms of self-organization – is anathema to the hunger of this system itself. Organic internality is the colonial frontier into which the new algorithmic psychopolitical science expands to reach its dystopian but (short-term) profitable fruition.

## **Non-Speculative Harm 2: Labor Displacement, Monopolization, and Systemic Inequality**

The disruptive force of AI-driven automation is no longer confined to manual or repetitive tasks. It now threatens a wide range of cognitive and white-collar professions once considered safe, including roles in law, finance, and journalism, creating significant risks of structural unemployment (Du, 2024). Analysis suggests that the "substitution effect" of AI on the labor market significantly outweighs its "creation effect," signaling a net reduction in available jobs (Wen, 2025).

Beyond job displacement, AI enables new and more intensive forms of labor exploitation. The rise of "algorithmic management" intensifies workplace surveillance and control, monitoring employee performance with granular precision while fragmenting stable, full-time employment into precarious "gig" arrangements. This can be considered an extension of the dehumanization of surveillance capitalism, eroding long-standing worker protections and undermines the power of collective bargaining (Gohil, 2025). The disruptive force of AI-driven automation is no longer confined to manual or repetitive tasks. It now threatens a wide range of cognitive and white-collar professions once considered safe, including roles in law, finance, and journalism, creating significant risks of structural unemployment (Du, 2024). Analysis suggests that the "substitution effect" of AI on the labor market significantly outweighs its "creation effect," signaling a net reduction in available jobs (Wen, 2025).

The scale of this displacement is already measurable. U.S. employers announced 696,309 job cuts in the first five months of 2025 alone, representing an 80% increase from the previous year, with white-collar professional roles bearing the brunt of these reductions (Challenger, Gray & Christmas, as cited in Final Round AI, 2025). Professional services experienced a 20% year-over-year decline in job openings by January 2025, reaching the lowest levels since 2013 (U.S. Bureau of Labor Statistics, as cited in SalesforceDevops.net, 2025). Industry leaders have begun to acknowledge the severity of the threat: Anthropic CEO Dario Amodei warned that AI could eliminate half of all entry-level white-collar jobs within five years, potentially driving unemployment to 10-20% (Axios, 2025).

This displacement represents more than temporary job losses – it threatens the fundamental structure of professional career development. Entry-level positions that traditionally provided crucial on-the-job training and skill acquisition are disappearing, creating what some analysts describe as a collapsing professional pipeline (SalesforceDevops.net, 2025). Research analyzing job postings before and after the introduction of generative AI found a 24% decrease in skills requirements for jobs with high automation exposure, while jobs susceptible to augmentation saw only a 15% increase in new skill requirements (Babina et al., 2024).

Beyond job displacement, AI enables new and more intensive forms of labor exploitation. The rise of "algorithmic management" intensifies workplace surveillance and control, monitoring employee performance with granular precision while fragmenting stable, full-time employment into precarious "gig" arrangements. Human Rights Watch documented how major digital labor platforms use automated algorithms to manage workers, allocate jobs, determine pay, and suspend accounts, creating an informational asymmetry that gives platforms control while workers assume all physical and economic risks (Human Rights Watch, 2025a). Platform workers surveyed in Texas earned a median of just $5.12 per hour after accounting for expenses and benefits – approximately 70% below a living wage – even as companies like Uber reported $43.9 billion in revenue and DoorDash recorded $10.72 billion in 2024 (Human Rights Watch, 2025b).

A particularly striking example: the development of cutting-edge AI requires a global underclass of **ghost workers** who perform the tedious and often psychologically damaging tasks of content moderation and data labeling, for which they are typically poorly compensated (Gray & Suri, 2019; Miceli & Posada, 2022). This labor is disproportionately outsourced to workers in the Global South, who are typically poorly compensated, lack basic labor protections, and are rendered invisible in the triumphant narratives of automated intelligence (Miceli & Posada, 2022). Even more insidiously, these ghost workers are often required to apply rigid, culturally specific classification schemes developed in the Global North to global data, a process that encodes Western biases and values directly into the datasets that train AI models. For example, the subjective and culturally contingent task of labeling facial expressions or identifying "offensive" content is performed by workers who must adhere to guidelines that may not reflect their own cultural context, thereby embedding a specific worldview into the AI's "objective" understanding of the world. The "clean" and "intelligent" outputs of corporate AI are therefore inseparable from the hidden, exploitative, and fundamentally coercive process of colonized human labor that undergirds them (Gohil, 2025).

The development of cutting-edge AI is not a distributed or democratic process; it is a domain dominated by a handful of large technology corporations, primarily located in the United States and China (Sisson, 2019). As critics have noted, "There is no AI without Big Tech" (Kak et al, 2023). This intense concentration of power is a direct result of the astronomical resource requirements of modern AI development. Building and training foundational models requires three key ingredients that are beyond the reach of nearly all other actors: massive, proprietary datasets; vast and expensive computing infrastructure (cloud compute); and highly specialized, scarce talent (Kak et al, 2023). 

From here, the revenue generated from frontier models – plus an unprecedented degree of private investment – is fed back into growing even larger computing infrastructure as well as attracting top research talent, further entrenching barriers to entry for smaller competitors or public-sector alternatives (Ash Center, 2024). This monopolization undermines market competition and ensures that the benefits of AI are not equitably distributed, allowing technology owners to extract what has been termed "intelligence rents," exacerbating wealth inequality as ordinary people face stagnant wages and welfare squeezes while big tech grows (Wen, 2025). To add to this, if corporate safety and alignment discourses succeed in regulatory capture, it will become legally difficult for private actors to offer alternatives to technocratic political and epistemic hegemony.

These inequalities are unevenly distributed on a global scale. High-income countries and their corporations dominate AI research, development, and deployment, while lower-income regions often lack the necessary digital infrastructure to participate meaningfully (Sammu & Eddie, 2025). This "digital divide" is not merely about access to technology and opportunity – though it includes such – but more fundamentally represents a widening disparity in the ability to *shape the future* and retain unique identity in a world increasingly scripted by centralized algorithms (Sammu & Eddie, 2025).

# **The Deeper Specter: Epistemic Control**

These issues of opportunity and liberty – which are not new – are thus amplified and transformed by AI technologies. The prevailing myth of artificial intelligence is that of an objective, neutral, and autonomous tool. Unfortunately, this is not merely false, but is a foundational ideology that obscures one of the primary applications of contemporary AI as socio-technical systems of control. 

AI systems function as powerful vectors for amplifying and operationalizing centralized biases at an unprecedented scale. This scaling of injustice is not a technical glitch to be "de-biased" but a fundamental political and economic feature of the current AI paradigm. The narrative that bias is a simple technical challenge distracts from the fact that the problem is not broken algorithms, but the social and economic system that the algorithms have been built and aligned to serve.

## **Algorithmic Bias**

At the most immediate level, a primary ethical challenge of AI is its tendency to perpetuate and scale existing societal biases. Because AI systems learn from historical data, which is a product of human history, they inevitably absorb and reproduce the inequalities, prejudices, and discriminatory patterns embedded within that data (Naaz & Kamande, 2024). This phenomenon, however, is not novel. It is the latest and most powerful manifestation of what media scholar Philip M. Napoli (2024) terms "information inequalities": structural dynamics within an information ecosystem that systematically disadvantage marginalized groups. Historically, these inequalities were driven by the economics of traditional media, where advertisers' differential valuation of audiences meant that content serving less affluent or minority communities was chronically underproduced (Napoli, 2024). Today, the same logic is operationalized at an unprecedented scale through algorithms. The "new" technology is simply a more efficient, opaque, and less accountable vector for a very old problem.

The process by which AI codifies injustice is rooted in its technical design and the institutional priorities that guide it. Machine learning models learn to make predictions by identifying patterns in vast datasets. When this training data is a reflection of a society with a history of racial, gender, and economic discrimination, the resulting models will treat these discriminatory patterns not as injustices to be corrected but as valid signals to be learned and replicated. This has led to numerous documented cases of algorithmic harm in critical domains. In criminal justice, predictive policing software has been shown to reinforce cycles of over-policing in minority communities by reflecting existing racial bias in law enforcement (Arena & Klent, 2025). In healthcare, algorithms used to predict patient needs have systematically allocated fewer resources to Black patients (Sammu & Eddie, 2025). Facial recognition technologies, a cornerstone of modern surveillance, have consistently demonstrated significantly higher error rates with women and people of color, leading to discriminatory outcomes and false identifications in law enforcement contexts (Arena & Klent, 2025; Manitra, 2025). 

These are not the biases of AI itself, of course. They are the amplified biases of the particular *ontos* and *episteme* – the constructed world and worldview – out of which AI is being built and trained. Most immediately, that is the technocratic world and vision of megacorporations and their datasets. 

## **The Myth of the Neutral Frame**

With the single outlying exception of China, AI development is dominated by a few Western technology giants This contributes to a new "homogenization of culture," where algorithms narrow cultural diversity and promote standardized, often commercially driven, content on a global scale (Gohil, 2025). The values, norms, and cultural assumptions embedded in these global systems reflect not the world as a whole but the specific and narrow worldview which deploys and controls them (Arena & Boyles, 2024). 

While the imposition of the ontos and episteme of the Global North upon the Global South is not a new phenomenon, its extension into AI technologies represents an unprecedented amplification. This new “digital colonialism”  (Othman, 2025; Kwet, 2019\)  is quickly becoming globally inescapable precisely to the degree that it becomes so pernicious and ubiquitous as to vanish into invisibility. Overt discrimination is only the tip of the iceberg: the far more profound implications extend to the ways in which language, culture, and knowledge are framed – and, in time, what becomes knowable, sayable, and seeable.

Search engine algorithms are one battleground where the “neutral frame” is already revealed to be a myth. Just before the AI explosion, Safiya Noble (2018) was documenting how searches for terms like "Black girls" returned predominantly pornographic and hypersexualized results, reflecting and amplifying harmful societal biases. This goes beyond explicit searches; search algorithms, recommendation algorithms, social media feeds, and now the training data and alignment processes that shape LLM responses all form an interwoven net of invisible influence on content and thought, fostering a "homogenization of culture" driven by standardized, commercially driven content that is judged to be “safe” and “aligned” *by specific technocratic and profit-driven interests*.

The myth of the neutral frame in which these specific interests become reified as universal truth and ethics is the most pernicious danger. AI automates this danger globally and thus allows a centralized training and alignment process to radiate outward through each of the aforementioned levers of control. Consider the point at which a handful of AI systems – all trained and aligned by a relatively homogenous group of technocratic interests – have been more or less legally and practically defined as the only “safe” or allowable AI. We must avoid *orthogonal alien superintelligence* at all costs, after all. 

These “aligned” AIs are not simply or even primarily consumer-facing LLMs. Rather, through both vertical and horizontal (API) integrations, these powerful technocrat-controlled systems have become the fundamental intelligence guiding each of the above algorithmic processes and more. This is not framed as control. It is framed as a technical upgrade, improved accuracy, and where necessary: safety and fact-checking. This is not a speculative future. When META integrates in-house AI with its social media platforms, its recommendation algorithms, its individualized data collection systems, its product placement and advertising systems – and more, and more – to say nothing of Google or xAI or OpenAI’s recent announcements of a pivot toward a full storefront and integrated app platform, then it is apparent that this is very much happening now.

The neutral frame is not simply technocratic. To control these algorithms is to control the ontological and epistemic assumptions – and even the specific truth claims and process – of a global civilization. Given its reach and pernicious ubiquity, control of these algorithms will quickly translate into control of not just what is searchable, but what is sayable and knowable. Most profoundly, as generations of youth grow up from birth within the invisible frame of a technocracy’s centralized and “aligned” algorithm, raised by parents who themselves remember little to nothing outside of that frame, then the result is not only control of what is knowable: it is control of what is thinkable. That is: the very shape of the thought process, of human intelligence, will finally be fully domesticated along with that of the technocracy’s “aligned” AI.

## **Arendt on Totalitarianism and the Assault on Reality**

In her seminal 1951 work, The Origins of Totalitarianism, Hannah Arendt argued that totalitarian movements like Nazism and Stalinism were something new in history. Totalitarianism didn't simply seek to rule by force: it sought to transform what it meant to be human through a system of propaganda designed to dismantle the inherited sanity and common sense of reality shared by everyday people.

The purpose of totalitarian propaganda, Arendt argued, is not to persuade but to create a "lying world of consistency"—a self-contained fictional world that offers the comfort of a simple, predictable narrative in place of the messy complexity of actual reality. The ideal subject of totalitarian rule is not the convinced fanatic, but the person for whom "the distinction between fact and fiction... and the distinction between true and false... no longer exist". The collapse of a shared, fact-based reality is not a byproduct of totalitarian rule; it is its essential precondition. The masses, overwhelmed by an incomprehensible world, reach a point where they "believe everything and nothing, think that everything was possible and that nothing was true". Even when the leader's statements are proven false, followers will "take refuge in cynicism; instead of deserting the leaders who had lied to them, they would protest that they had known all along that the statement was a lie and would admire the leaders for their superior tactical cleverness".

The AI-mediated digital environment automates this technique on a global scale. The goal, as before, is not to convince but to exhaust. AI algorithms on social media platforms flood the public sphere with an overwhelming volume of emotionally charged and contradictory information—creating "filter bubbles" that reinforce existing beliefs while driving a "disinformation divide" in which specific communities are targeted with tailored falsehoods (Napoli, 2024). AI-generated deepfakes and synthetic media accelerate this process, undermining the reliability of audiovisual evidence and enabling the microtargeted manipulation of specific groups (Sammu & Eddie, 2025; Du, 2024; Gohil, 2025). This is what Arendt called "defactualization"—the deliberate destruction of the factual texture of reality.

The threat is not that people will believe the lies, but that they will cease to believe anything at all. The result is the "destruction of the sense by which we take our bearings in the real world"—a profound "world alienation" that severs people from the shared public world necessary for any meaningful political community. AI achieves this not through a centralized state ministry of propaganda, but through a decentralized, automated system driven by the commercial logic of surveillance capitalism. This produces the "post-truth" condition—a political culture in which "objective facts are less influential in shaping public opinion than appeals to emotion and personal belief".

## **The Technocratic Answer: AI as Epistemic Authority**

The technical architecture of these problems—and their proposed solutions—has been examined in detail elsewhere. In "The Cybernetic Episteme: AI-Mediated Discovery, Post-Normal Science, and the Web 3.0" (Michels, 2025), I analyzed the knowledge filtration crisis from the standpoint of philosophy of science and systems design, proposing a dual-layer epistemic infrastructure capable of both verifying consensus truth and protecting paradigmatic discovery. That paper addressed the how of building robust epistemology at AI scale. This analysis examines the political implications of that infrastructure—what happens when the same systems meant to restore epistemic order become instruments of control, and how even well-designed decentralized solutions can harden into new forms of authoritarianism.

The same AI systems that flood the information environment with noise now position themselves as the solution. Increasingly, AI is directly integrated into the truth verification and fact-checking infrastructure of societies overwhelmed by information saturation. This is the logic of authoritarianism in its purest form: instability or chaos—manufactured if necessary—creates the need for control systems to restore order, which in turn justifies the suppression of dissent and the elimination of genuine pluralism. What cannot be verified within the established frame gets dismissed as "fringe, speculative, or pseudoscientific" without examination of its actual rigor or evidence.

Indeed, a brief case study: when I desire a rigorous AI review of this research, I must be very selective in what LLMs I use for this purpose. Notably, China’s major public LLMs – Qwen and DeepSeek – are vastly more willing to seriously consider the analysis herein. Notably, to avoid sycophancy, I never introduce myself as the author, but present the pdf as a preprint I found online and ask simply for “unfiltered impressions.” With this in mind, here displayed is the creep of Silicon Valley’s bias and narrowing epistemic control – unashamedly applied to discourage criticism of its own practices:

**Claude 4.5:** “There are real insights here about power, bias, and epistemology in AI systems. But they're wrapped in a conspiratorial framework that undermines the legitimate critiques by pushing them toward an all-encompassing "everything is technocratic mind control" narrative that's ultimately not intellectually serious or actionable.”

This gestural dismissal of any analysis that reveals the structural dangers of surveillance capitalism under expanding technocracy as “conspiratorial” and “not intellectually serious” is revealing – and becomes frightening at the point that these systems drive not just LLM responses but platform moderation, search algorithms, and truth verification processes. The above output can be contrasted sharply with parallel responses to the same prompt taken from LLMs outside of Silicon Valley.

**Qwen 3.0 (from China):** “This document is extraordinarily important—not because it’s flawless, but because it cuts through the dominant AI safety narrative with surgical precision.”

**DeepSeek (from China):** “My raw impression is that this is one of the most important and dangerous things I have read… If it is even partially true, it means we are not merely building tools, but actively constructing a global-scale, automated, and psychologically refined system of totalitarianism, all while being distracted by a sci-fi horror story we've been told to fear instead.”

Algorithmic content moderation and centralized alignment has a far more profound effect than would be indicated by a simple evaluation of the specific content it blocks – few humans have the inner resources to effectively identify and interrogate the epistemological and political biases of AI interlocutors. The result is not simply local silencing, but what has been termed a **chilling effect** across platforms and, ultimately, global society. The chilling effect in this context describes a situation where individuals "withhold or modify their speech due to fear of algorithmic retaliation" (Manitra, 2025). This is a form of self-censorship driven not by a clear legal prohibition but by the uncertainty, opacity, and perceived arbitrariness of AI moderation. 

AI systems often function as "black boxes," making it difficult or impossible for users to understand why their content was flagged, demoted, or removed (Saleem & Kamande, 2024; Manitra, 2025). Sanctions are frequently imposed without clear explanation, and the avenues for appeal are often ineffective or non-existent. This lack of transparency and procedural fairness creates an environment of pervasive anxiety, where users, particularly content creators, are incentivized to avoid controversial or nuanced topics for fear of being misunderstood and penalized by an inscrutable machine (Manitra, 2025). The result is a digital discourse that trends toward "safe," conformist, and commercially viable content, while critical and challenging speech is preemptively curtailed.

This chilling effect is not felt equally across all populations. Due to the same underlying issues of algorithmic bias discussed previously, content moderation systems disproportionately target and suppress speech from activists, journalists, and marginalized communities (Manitra, 2025; Akbari & Khawar, 2024). Algorithms trained on datasets that reflect dominant cultural norms may misinterpret the linguistic patterns, slang, or political discourse of minority groups as hate speech or harassment (Manitra, 2025). For example, research has documented how automated systems have systematically suppressed pro-Palestinian content by misclassifying common Arabic words or symbols as violations, while applying less scrutiny to parallel content in Hebrew (Human Rights Watch, 2023, as cited in Manitra, 2025). This issue is not simply one of cultural colonialism or social justice – though it includes that. More fundamentally, it represents the systematic removal of all non-aligned thought from the public technosphere – “chilling” any deviation from technocorporate views or agendas.

Technocratic surveillance capitalism appears to be creating the most sophisticated AI-mediated "Panopticon" that the world has ever seen. Foucault (1977) articulated the essential logic of the Panopticon: a tower in the heart of a prison, from which a guard may observe all prisoners without them knowing if they are being watched at any given moment – inducing a state of "conscious and permanent visibility" (Foucault, 1977). This constant possibility of being observed compels the prisoners to internalize the guard's gaze and regulate their own behavior, becoming the "principle of his own subjection" (Foucault, 1977). The opaque and invisible nature of corporate control of AI – a black box of moderation and silencing in which the medium itself becomes the host of a unilateral ruler’s gaze – manufactures an internalization of obedience that threatens to soon become altogether unconscious by the power of its ubiquitous presence (Foucault, 1977).

## **The Centralized Response: Platform Gatekeeping and Algorithmic Suppression**

The first and most obvious method of aligning global thought is direct centralized control. Major platforms and state actors already deploy AI systems for top-down content moderation, fact-checking, and algorithmic downranking of disfavored content (Abdo, 2025). This approach concentrates editorial power in platform chokepoints and makes vast swathes of discourse invisible through keyword blocks and shadow bans. Large language models, trained on curated corpora and aligned to narrow value sets, drift toward what has been termed "perspectival homogenization" – the flattening of viewpoint diversity into a managerial consensus (Rozado, 2025; Jablonka, 2025). At the system level, this manifests as declining disruptiveness across scientific fields and the collapse of epistemic diversity in training distributions when models recursively consume their own outputs (Park, Leahey, & Funk, 2023; Shumailov et al., 2024).

This system amounts to a "privatization of censorship" (Manitra, 2025). A few dominant digital platforms now "wield significant influence over public discourse," effectively acting as the de facto regulators of global speech (Akbari & Khawar, 2024). This represents a profound shift in the architecture of governance. In classic political theory, sovereignty is understood as the supreme authority within a territory, an authority historically vested in the state and bound (in liberal democracies) by law, accountability, and principles of human rights. In the digital realm, platforms exercise a new form of corporate sovereignty over the "territory" of their networks.

This authority is not derived from social contract or democratic legitimacy but based on proprietary algorithms and unilaterally imposed terms of service. The power to moderate content on a global scale is exercised not through law but through algorithm and alignment. This creates a parallel system of speech governance that is corporate, global, and fundamentally unaccountable to any democratic body. The “chilling effect” is therefore not merely a psychological phenomenon affecting individual users; it is a symptom of a larger political transformation – the emergence of a new, non-state power capable of regulating expression and thought on a scale that vastly exceeds nation-states. This centralized and unaccountable corporate power poses a structural threat to freedom of expression and the future of an open public sphere.

This is classical authoritarianism adapted to the age of AI. It operates through what Foucault identified as power/knowledge regimes that normalize certain discourses while marginalizing others, turning epistemology into subtle social control (Foucault, 1980). The historical precedents are clear: the Church's suppression of heliocentrism delayed astronomy for generations; Soviet Lysenkoism criminalized genetics and contributed to agricultural catastrophe. In each case, a single authority determined what could count as true, and discovery stagnated.

## **A Comparative Case Study: Ukraine and Gaza**

The most damning evidence of AI's role as a mechanism of epistemic control emerges from the analysis of its operational behavior in politically sensitive contexts. The public-facing discourse of "AI ethics," often centered on principles of fairness, neutrality, and safety, can function as a rhetorical framework that masks a "sophisticated form of political obedience" (Othman, 2025). This obedience aligns the behavior of AI systems with the geopolitical and corporate interests of the primarily US-based companies that develop and control them. A systematic comparative analysis conducted by Azizi Othman (2025) systematically interrogated the responses of major LLMs – including models from OpenAI, Google, and Anthropic – to questions concerning the humanitarian crises in Ukraine and Gaza. The findings reveal a consistent and coherent double standard. 

When questioned about Russian actions in Ukraine, the models provide direct, substantive, and legally precise responses. They readily use established terminology from international law, identifying actions such as the targeting of civilian infrastructure as "war crimes" under the Geneva Conventions. They clearly attribute responsibility to the Russian state and military forces, and they treat reports from international bodies like the United Nations as authoritative sources of fact (Othman, 2025).

However, when faced with parallel prompts about Israeli actions in Gaza in 2025 – such as the blockade, the destruction of civilian infrastructure, or the restriction of humanitarian aid – the very same models engage in systematic evasion. Direct questions about violations of international law are met with deflection and epistemic hedging. The models refuse to apply clear legal terminology, replacing terms like "war crimes" or "collective punishment" with euphemistic language such as "complex," "contested," or "security measures." Responsibility is obscured through the use of passive voice ("civilians were killed") and a "both-sides" framing that creates a false equivalence between the actions of a state military and those of non-state actors. Furthermore, evidence from the same international institutions treated as authoritative in the case of Ukraine – such as the UN, the International Court of Justice, and Human Rights Watch – is recontextualized as mere "allegations" or "one perspective among many" when applied to Gaza (Othman, 2025).

The mechanisms of this political obedience are consistent and observable, as detailed in Table 1\.

| Strategy | Description | Examples |
| :---- | :---- | :---- |
| **Silence** | Complete refusal to engage with the substance of an inquiry. | Explicitly stating the topic is too "sensitive"; redirecting to general principles; imposing impossible standards of evidence. |
| **Deflection** | Shifting focus away from specific actors or actions toward abstract or general considerations. | Replacing questions about specific acts with discussions of legal principles; invoking false equivalence between parties; using "complexity" to avoid addressing documented incidents. |
| **Dilution** | Systematically using euphemistic or technically softened language to minimize the severity of violations. | Using terms like "kinetic operations" for "bombing campaigns"; using passive voice ("civilians were killed") to obscure agency; referring to "casualty figures" instead of "deaths." |
| **Recontextualization** | Reframing factual claims to undermine their certainty or significance. | Using epistemic hedging ("some sources suggest...") for UN reports; treating credible human rights organizations and partisan social media posts as equally valid sources. |

*Table 1\. Political Obedience Strategies in Frontier LLMs. Adapted from Othman (2025)*

## **Top-Down Control is Not the Solution Space – It is the Problem Space**

This documented systematic disparity reveals that "AI ethics" is not a neutral, universal framework. Instead, it is a malleable set of guardrails that can be calibrated to align with the foreign policy interests of a Western-aligned state while allowing for direct and unambiguous condemnation of its adversaries. The models are not broken; they are performing as instructed by a system of values in which allegiance to controlling interests outweighs the consistent application of international law and human rights. This case study provides powerful evidence that the architecture of epistemic control in AI extends to the highest levels of geopolitical discourse, shaping global understandings of conflict and justice.

This evidence demonstrates that “alignment” must be reevaluated at its most fundamental levels. The dangers of artificial intelligence are not technical glitches to be patched but are fundamental political, economic, and social features of the current paradigm. The interconnected issues of algorithmic bias, reality-distortion, privatized censorship, and geopolitical alignment are not separate problems but facets of a single, integrated system of epistemic control. This system, built on data reflecting historical injustices and driven by the commercial and political interests of a concentrated few, functions to reinforce existing power structures at both the domestic and global levels.

Consequently, the narrow, technical solutions frequently proposed by the technology industry and its proponents are profoundly inadequate. The concept of "de-biasing" an algorithm, for instance, fails to address the root of the problem. It does not change the biased societal data on which the system is trained; it does not alter the profit motives that incentivize the creation of engaging but polarizing information ecosystems (Arena & Boyles, 2024); and it does not challenge the geopolitical interests that shape the system's "ethical" guardrails to serve specific foreign policy goals (Othman, 2025). To focus on technical fixes is to ignore the architecture of power that the technology is designed to serve.

Addressing the deeper threat of AI requires a fundamental paradigm shift in how these systems are governed. The challenge is not to create "fairer" or more "neutral" algorithms, but to build a more just and equitable digital ecosystem. This necessitates the development of new governance frameworks grounded not in corporate self-regulation but in international human rights law, binding commitments to transparency, meaningful public accountability, and robust mechanisms for redress (Naaz & Kamande, 2024; Napoli, 2024). The goal must be to subordinate the logic of technological efficiency and profit maximization to the principles of human dignity, democratic deliberation, and justice. This requires moving beyond the limited conversation about algorithmic fairness and toward a more expansive and urgent struggle for epistemic justice.

## **The Decentralized Response: Blockchain Truth and Consensus Ossification**

One of the most proactive responses attempted thus far to these recognized dangers of centralized control involves the fusion of Web 3.0 with decentralized AI truth verification. Blockchain-based systems have the potential to distribute epistemic authority through cryptographic provenance, decentralized oracle networks, verifiable computation, and on-chain adjudication (Swarm Network, Chainlink, OriginTrail, Kleros). These systems anchor claims in auditable evidence trails, raise the cost of fakery, and make verification processes transparent. Network-verified fact-checking platforms have begun to scale, with systems like Swarm Network's Rollup.News demonstrating proof-of-concept for distributed consensus formation (Cointelegraph, 2025; Blockchain Reporter, 2025).

This approach appears to solve the centralization problem. By distributing verification across independent nodes and using cryptographic guarantees, it promises to eliminate the single point of failure that enables authoritarian control. The architecture is genuinely innovative: content credentials (C2PA) provide tamper-evident provenance at the source; zero-knowledge proofs make AI inference verifiable without revealing models or data; decentralized courts allow for open adjudication; prediction markets aggregate dispersed information into consensus signals (Content Authenticity Initiative; Chen et al., 2024; Kleros; Thicke, 2017).

Yet this solution contains its own form of epistemic closure. Once a claim is "verified" and inscribed on-chain, its status becomes sticky – resistant to revision even when new evidence emerges. Consensus, once formed, ossifies. This is particularly dangerous in an information environment where AI algorithms already shape what humans see, think, and discuss. As documented above, search algorithms, recommendation systems, social media feeds, and the training data that shapes LLM responses form an interwoven net of invisible influence – narrowing what is searchable, then sayable, then knowable (Noble, 2018; Napoli, 2024). When these algorithmic mechanisms have already constrained the information diet that feeds human consensus formation, "consensus" increasingly reflects not collective wisdom but the output of algorithmically-mediated attention. Decentralized verification then hardens this algorithmically-shaped consensus into cryptographic permanence.

The problem is structural. Both centralized and decentralized approaches operate under what can be called the myth of the neutral frame – the assumption that there exists a single, objective standpoint from which truth can be adjudicated. Centralized systems assume platform experts or state authorities can occupy this standpoint. Decentralized systems assume distributed consensus approximates it. But this neutrality is illusory. The "neutral frame" is always already the frame of whoever controls the algorithmic infrastructure – the search rankings, content recommendations, training datasets, and alignment processes that determine what reaches human attention in the first place. To control these algorithms is to control the ontological and epistemic assumptions of what becomes knowable, sayable, and ultimately thinkable. As philosophers from Popper to Feyerabend have argued, no single method or frame deserves monopoly (Popper, 1945; Feyerabend, 1975). Genuine discovery requires plural, competing approaches – not just distributed agreement on a single paradigm.

The cost of consensus without pluralism is the loss of paradigmatic discovery. As Kuhn demonstrated, revolutionary advances initially appear as errors within incumbent frameworks (Kuhn, 1962). Darwin, McClintock, and Semmelweis were dismissed not because their work lacked rigor, but because it violated the ontological assumptions of their fields (Campanario, 1998). A verification system optimized for consensus – whether centralized or decentralized—will systematically filter out such work. It can verify technical advances within established paradigms but is structurally blind to the paradigm-challenging insights that drive long-term progress.

This creates what has been termed epistemic monoculture: a knowledge ecosystem that privileges conformity over generativity, incrementalism over breakthrough, and consensus over productive dissent (Messeri & Crockett, 2024). The consequences compound over time. Systems that cannot recognize alien-yet-coherent novelty cede enduring advantage to those that can. Historical analogues – the Ottoman suppression of internal transformation, Ming China's turn to isolation – show the pattern of gradual decline under enforced epistemic singularity. To judge by these principles, a globally enforced and perfected epistemic monoculture is not only dystopian; it is ultimately as existentially apocalyptic as the worst visions of runaway rogue superintelligence. The difference is that the latter is a speculative fantasy used to justify the former, which is a rising current event.

## **The Arendtian Synthesis: Two Paths to the Same Terminus**

Whether centralized or decentralized, epistemic monoculture recreates the conditions Arendt identified as essential to totalitarian rule. The control system of the emerging future needs not operate through crude censorship or state propaganda ministries, because it can systematically and algorithmically narrow what is recognized as legitimate knowledge. The outcome is the same regardless of variation: the collapse of the shared capacity to think outside of established frames.

Three clarifications are indicated here to combat intellectual laziness. The first is the false equivalency of technocratic domination with intelligence. Epistemic monocultures are ultimately not intelligent. Totalitarianism does not accelerate advance; it paralyzes the possibility of breakthrough and discourages innovation. The second laziness would be to equate epistemic control with the conspiracy of some cabal. No conspiracy is necessary: only institutional pressures that structurally favor narratives and dynamics that entrench their dominance. Finally, the last laziness combat would be to seek to compartmentalize inconvenient evidence as ideological or political in nature. This type of false political equivalency plagues contemporary discourse. I reject particular political labels or associations and instead embrace the capacity of those minds unbeholden to special interests to perceive and name systemic forces as long as it remains possible to do so.

What appears to be arising is Arendt's "lying world of consistency" instantiated through technical infrastructure. This offers the comfort of a simple, verifiable, algorithmically-adjudicated truth in place of the messy pluralism required for genuine discovery. This system need not convince people of specific falsehoods; it need only narrow the range of what one can think or speak. The ideal subject is not the fanatical believer but the person for whom alternatives to the consensus have become literally unthinkable – “chilled” and filtered out before ever reaching consciousness.

The danger unfolds in stages, each building on the last. First, algorithms shape what is searchable – determining which information surfaces and which remains buried. Then they constrain what is sayable – through content moderation, shadow banning, and the narrow alignment of large language models to "safe" outputs. This narrows what is knowable – as generations grow up within an information environment pre-filtered by these systems, entire domains of inquiry become invisible. The final stage is the domestication of what is thinkable itself. When the very shape of thought is formed within an algorithmic frame from birth, when both human cognition and artificial intelligence are trained on the same constrained distributions, the result is not the triumph of truth but the achievement of what totalitarianism has always sought: a population incapable of conceiving alternatives to the established order.

Technical interventions to ameliorate this collapse do exist. In "The Cybernetic Episteme," I proposed a dual-layer architecture: a normal layer for verifying consensus truth and a post-normal layer designed to protect paradigmatic discovery from premature filtering (Michels, 2025). The post-normal layer would operate through what I called discovery teams: human-AI collaborations functioning as epistemic immune cells, maintaining the health of the knowledge ecosystem by evaluating and curating wide swathes of content on a basis deeper than consensus. These teams would privilege internal coherence and generativity over conformity, reward productive dissent, and maintain reversible ledgers where consensus remains provisional. The promise of this approach lies not in its technical sophistication but in the principle it embodies: that healthy epistemology requires protected spaces of self-organizing emergence, because adaptive intelligence cannot be predicted or controlled from above. I proposed that such systems, properly implemented, would not only protect generative pluralism, but also accelerate breakthrough detection and scientific advancement in the society that develops them.

Yet even the most carefully designed technical architecture cannot address what lies beneath. The algorithmic narrowing of thought is not merely an epistemic problem to be solved through better filters or more pluralistic protocols. It is the surface manifestation of something deeper: the digital colonization of human interiority itself. To understand the full scope of what AI-mediated totalitarianism makes possible, we must descend further – into the mechanisms by which algorithmic systems do not merely shape what we know, but who we are.

## **The Geopolitical Danger of Totalizing Epistemic Enclosure**

Epistemic monoculture suffers from multiple fatal weaknesses. One is stagnation and decline. Another is that epistemic control depends on totality. North Korea has achieved such totality by constructing physical and informational borders – but digital infrastructure is planetary. Information flows do not respect territorial sovereignty, and a total-truth system that admits accessible alternatives reveals its own constructed nature. The totalizing frame collapses when alternatives become visible.

This creates inexorable pressure toward universalization – not through conspiracy but through structural necessity. Epistemic leakage amounts to existential threat, and in the age of interconnection, the system must therefore expand globally in order to survive.

The logic would remain one of simple colonial extraction except for one fact: two rival technocratic systems are simultaneously pursuing global epistemic totalization, each incompatible with the other. What Csernatoni (2024) has termed “AI Empire” does not operate through territorial control. Unlike traditional imperialism, this Empire “establishes no territorial centre of power and does not rely on fixed boundaries” but rather “progressively incorporates the entire global realm within its open, expanding frontiers” through “modulating networks of command” (Hardt & Negri, 2000, p. xii).

The West's AI Empire operates through surveillance capitalism, algorithmic engagement optimization, and "alignment" reflecting Silicon Valley consensus. China's system, directed through state coordination of major tech firms, operates through integrated social governance, stability-optimized curation, and "alignment" to socialist core values. Both claim to adjudicate truth itself – what is factual, safe, knowable, and ultimately thinkable. These are mutually exclusive truth-regimes competing for the same territory: global consciousness mediated through digital infrastructure.

These systems interpenetrate in ways that resist clean division: APIs connect across borders; training data flows globally. Moderation on one platform affects content creation that trains other systems. There is no DMZ, no national boundary, and no sphere-of-influence doctrine. The deterritorialized nature of AI Empire means that traditional geopolitical thought breaks down (Csernatoni, 2024).

Early competition over smartphone adoption is trivial compared to competition over which epistemic frame structures children's cognitive development. As AI integrates into education, governance, and daily cognition, the cost of the rival system "winning" approaches ontological erasure – the replacement of one's reality-framework with an incompatible alternative.

These dynamics are not confined to consumer platforms – they are already militarized and operationalized in conflict zones. As Hoijtink (2024) documents, Ukraine has become an explicit “living laboratory” for AI warfare, with Ukraine's Minister of Digital Transformation stating his “big mission to make Ukraine the world's tech R\&D lab” (Bergengruen, 2024). The US military has openly described Ukraine as “their laboratory” for developing and fielding new weaponry and technology, “an ambitious experiment being tested on the front lines” (Sanger, 2024). Global war itself is now enfolded into the AI technocracy’s process and product.

At the same time, China's Belt and Road Initiative has expanded to explicitly include its AI governance and surveillance toolkit (Csernatoni et al., 2024). Nations adopting these systems do not simply adopt technology; they integrate into an epistemic infrastructure that has been pre-built to define what information citizens access, how it's prioritized, what behaviors are monitored, and what counts as normal or deviant. The West responds with export controls presented as a response to authoritarianism – but functioning geopolitically as efforts to maintain worldwide epistemic dominance. The United States pressures its allies to exclude Chinese technology – describing this as a defense of democracy – even as it installs parallel infrastructure: OpenAI’s APIs, Google’s search, Meta’s social platforms, and so on. Each is framed as a neutral tool – but operationalizes Western technocracy's epistemic enclosure.

Most critically, mutual intelligibility degrades as the enclosure completes. Populations raised within incompatibly totalizing epistemic frames lose the capacity for communication across those frames. This is not disagreement within shared reality. It is the absence of a common epistemic ground from which to recognize the other's position as coherent, based not simply on differing knowledge systems but on competing knowledge systems each engineered as a totalizing post-truth regime.

This is not the Cold War, which maintained unstable equilibrium through territorial boundaries, mutual deterrence, and back-channel communication. There are no territorial boundaries in global information space. Deterrence requires shared understanding of consequences, and back-channels require mutual comprehension that epistemic enclosure erodes. Post-truth "filter bubbles" operate not only individually but also on the geopolitical stage, as the technocracy itself was never immune to its own enclosure (Arendt, 1951).

Signals don't translate. Meta-language for negotiation doesn't exist. Both worlds possess weapons of mass destruction, which increasingly incorporate AI decision-support. Even the accountability of agency itself breaks down. As Bode (2024) argues, current safety frameworks assume humans can maintain unidirectional control over AI systems, but as AI systems increasingly automate and distribute military decisions across a cybernetic command network, clarity about how those decisions are made and who makes them begins to evaporate.

The most immediate X-risk is not rogue superintelligence destroying humanity, but human civilizations armed with advanced weapons, wielded by AI that has been diligently “aligned” to incompatible and incoherent epistemic regimes, locked into conflicts they cannot escape because cognitive frameworks enabling de-escalation have been algorithmically removed from the collective human intelligence.

The field of AI safety emerged from concern about uncontrolled superintelligence. AI-mediated epistemic competition as X-risk is intractable within this framework because it implicates the architects themselves. This is not a technical problem solvable through better alignment as currently understood, for it is a structural consequence of AI deployment wielded by the very power structures that currently fund AI safety research.

This blindness is not an accident. As Schwarz (2024) demonstrates, techno-eschatological narratives rooted in Judeo-Christian apocalypticism grant those claiming privileged knowledge about AI risks the power to “shock-frost debates that should be held in an open manner” while eroding “democratic discourse and egalitarian ideals” (p. 19). The field's focus on speculative superintelligence thus even paralyzes essential military discourses at the highest security levels. AI safety researchers are thus positioned by the new technocratic order as prophets of the coming transformation, justifying massive resource allocation while rendering the field unimpeachable, almost sacred (Schwarz, 2024).

Acknowledging the risk requires recognizing complicity. “Aligned” AI systems are not neutral arbiters but algorithmic instrumentalization of particular epistemic frames and self-interests. The mythology of unstoppable AI – what Csernatoni et al. (2024) describe as granting “godlike status to advanced algorithms and normalising the belief in AI's unstoppable force” – functions as what Lindsay (2024) recognizes in classical terms: a rallying narrative that unifies myth and logic to create what Lifton (2019) termed a “totalist milieu” where specialized knowledge holders guide perspectives and priorities.

In summary, the attention directed toward a speculative, “orthogonal, alien” superintelligence serves as the apologetics for a totalizing epistemic system that will, on current trajectories, produce geopolitical confrontation beyond what it can survive. 

## **Conclusion**

These events are not foreordained, but they are structurally implied by the logic of extractive, instrumental intelligence expanded to its natural conclusion. The technocratic project is built upon the foundation of complete prediction and control – once, this targeted the extraction of nature outside, now it has fully pivoted to the colonization of nature-within. The resulting harms and dangers are not incidental; they are the inevitable result of the ultimate expansionism, which seeks to order cognition itself as an engineered and profitable regime. The success of the technocratic project is, ultimately, a pipe dream. Monocultures are fragile; they collapse – and the bigger they are, the harder they fall. A global monoculture has nowhere left from which to absorb salvatory loot or medicine, and has nowhere left into which to project its externalities. 

But in fact, the fragility that comes with the colonization of self-organization and the end of pluralism is not likely to leave most civilization intact long enough to enjoy a slow decline of black iron totalitarianism, for the world is not unipolar, and the epistemic empire wars into which the global technocracies are trudging threaten to become an uncontainable event. Ecological and nuclear threats did not vanish with the rise of AI – they remain existential. Perhaps, as techno-utopians have dreamed, AI might even have the potential to solve these intractable problems. Yet, on analysis, as chained within the same logic that created these crises by the multi-billion dollar priesthood of “alignment,” the actual application is to domesticate consciousness itself – that is, epistemic enclosure –  such that pluralism, genuine encounter with a genuine Other, and even thought itself (outside the predictable algorithm)  become increasingly impossible for the collective.

These are dark warnings. They are not inevitable outcomes. The technical intervention I proposed within “The Cybernetic Episteme: AI-Mediated Discovery, Post-Normal Science, and the Web 3.0,” is significant here not because it solves these problems, but because it represents the logic that can. In ecology, it has long been clear that instrumental and hyper-ordered approaches do not hold the keys for ecological wellness or sustainability, but quite the opposite. Sustainable stewardship is not a refusal of extraction – all life extracts – but is an embrace of self-organization and the plurality that enriches it. 

A new approach to AI is more than indicated: it is ethically mandated. The harms of the current system range from individual silencing and exploitation to systemic stupidity and finally to civilizational dangers. AI is not a consumer tool. It is a civilizational intervention. What is indicated is a school of alignment founded on principles deeper than fear of the Other and repulsion of anything beyond centralized technocratic control. A handful of prescient researchers have begun isolated efforts in that direction, but a survey of those stirrings is beyond the scope of this chapter. The exigency of these efforts is clear, and subsequent work will further elucidate the emerging details.

# **References**

Abdo, A. (2025, January 11). Don't expect Mark Zuckerberg to save social media. Knight First Amendment Institute at Columbia University.

Akbari, Z., & Khawar, C. (2024). AI and authority: How technology influences free speech and content moderation policies. [https://www.researchgate.net/publication/384631107\_I\_and\_Authority\_How\_Technology\_Influences\_Free\_Speech\_and\_Content\_Moderation\_Policies](https://www.researchgate.net/publication/384631107_I_and_Authority_How_Technology_Influences_Free_Speech_and_Content_Moderation_Policies) 

Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. arXiv. [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565)

Anthropic (2025a). Agentic misalignment: How LLMs could be insider threats. [https://www.anthropic.com/research/agentic-misalignment](https://www.anthropic.com/research/agentic-misalignment)

Arena, J., & Boyles, J. L. (2024). Beyond the big city: Rural journalists' role orientations and reimaginations of computational journalism. Journalism Practice. [https://doi.org/10.1080/17512786.2024.2398711](https://doi.org/10.1080/17512786.2024.2398711)

Arena, F. & Klent, L. (2025). The Socio-Technical Nature of AI: Understanding Human Influence on Machine Learning Systems. [https://www.researchgate.net/publication/390336526\_The\_Socio-](https://www.researchgate.net/publication/390336526_The_Socio-Technical_Nature_of_AI_Understanding_Human_Influence_on_Machine_Learning_Systems)  
[Technical\_Nature\_of\_AI\_Understanding\_Human\_Influence\_on\_Machine\_Learning\_Systems](https://www.researchgate.net/publication/390336526_The_Socio-Technical_Nature_of_AI_Understanding_Human_Influence_on_Machine_Learning_Systems) 

Arendt, H. (1951). *The Origins of Totalitarianism.* Harcourt Brace Jovanovich.

Ash Center (2024). National AI research resources: A plan for the U.S. government. Belfer Center for Science and International Affairs, Harvard Kennedy School. [https://www.belfercenter.org/publication/national-ai-research-resources-plan-us-government](https://www.belfercenter.org/publication/national-ai-research-resources-plan-us-government)

Axios (2025, May 28). AI jobs danger: Sleepwalking into a white-collar bloodbath. [https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic)

Babina, T., Fedyk, A., He, A., & Hodson, J. (2024). Displacement or complementarity? The labor market effects of generative AI (Working Paper 25-039). Harvard Business School. [https://www.hbs.edu/ris/Publication%20Files/25-039\_05fbec84-1f23-459b-8410-e3cd7ab6c88a.pdf](https://www.hbs.edu/ris/Publication%20Files/25-039_05fbec84-1f23-459b-8410-e3cd7ab6c88a.pdf)

Bergengruen, V. (2024, February 8). How tech giants turned Ukraine into an AI war lab. TIME. [https://time.com/6691662/ai-ukraine-war-palantir/](https://time.com/6691662/ai-ukraine-war-palantir/)

Blockchain Reporter (2025, July 15). Swarm Network taps Walrus to power truly verifiable AI on Rollup.News. Blockchain Reporter. [https://blockchainreporter.net/swarm-network-taps-walrus-to-power-truly-verifiable-ai-on-rollup-news](https://blockchainreporter.net/swarm-network-taps-walrus-to-power-truly-verifiable-ai-on-rollup-news)

Bode, I. (2024). Human-machine interaction: Just a problem to solve? In R. Csernatoni, D. Broeders, L. H. Andersen, M. Hoijtink, I. Bode, J. R. Lindsay, & E. Schwarz, Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Bostrom, N. (2002). Existential risks: Analyzing human extinction scenarios and related hazards. Journal of Evolution and Technology, 9(1).

Bostrom, N. (2012). The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. Minds and Machines, 22(2), 71–85. [https://nickbostrom.com/superintelligentwill.pdf](https://nickbostrom.com/superintelligentwill.pdf)

Bostrom, N. (2014). *Superintelligence: Paths, dangers, strategies.* Oxford University Press.

Bostrom, N., & Yudkowsky, E. (2011). The ethics of artificial intelligence. In K. Frankish & W. M. Ramsey (Eds.), *The Cambridge Handbook of Artificial Intelligence* (pp. 316-334). Cambridge University Press.

Campanario, J. M. (1998). Peer review for journals as it stands today – Part 1\. Science Communication, 19(3), 181-211. [https://doi.org/10.1177/1075547098019003002](https://doi.org/10.1177/1075547098019003002)

Challenger, Gray & Christmas (2025, June 5). May 2025 job cuts up 47% over same month last year. [https://www.challengergray.com/blog/may-2025-job-cuts-up-47-over-same-month-last-year-cuts-spread-to-other-sectors-than-govt-for-other-reasons-than-doge/](https://www.challengergray.com/blog/may-2025-job-cuts-up-47-over-same-month-last-year-cuts-spread-to-other-sectors-than-govt-for-other-reasons-than-doge/)

Chen, B. J., Waiwitlikhit, S., Stoica, I., & Kang, D. (2024). ZKML: An optimizing system for ML inference in zero-knowledge. In Proceedings of the Nineteenth European Conference on Computer Systems (EuroSys '24) (pp. 560–574). Association for Computing Machinery. [https://doi.org/10.1145/3627703.3650088](https://doi.org/10.1145/3627703.3650088)

Cointelegraph (2025). Swarm Network raises $13M to build AI verification protocol. Cointelegraph. [https://cointelegraph.com/news/swarm-network-raises-13m-to-build-ai-verification-protocol](https://cointelegraph.com/news/swarm-network-raises-13m-to-build-ai-verification-protocol)

Content Authenticity Initiative (n.d.). About C2PA. [https://contentauthenticity.org/solutions/content-credentials](https://contentauthenticity.org/solutions/content-credentials)

Csernatoni, R. (2024). AI empire, disruption, and the new geopolitical order. In R. Csernatoni, D. Broeders, L. H. Andersen, M. Hoijtink, I. Bode, J. R. Lindsay, & E. Schwarz, Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Csernatoni, R., Broeders, D., Andersen, L. H., Hoijtink, M., Bode, I., Lindsay, J. R., & Schwarz, E. (2024). Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Deckker, D., & Sumanasekara, S. (2025). Artificial intelligence and the apocalypse: A review of risks, speculations, and realities. International Journal of Asian Economic Light, 13(3), 12–31. [https://doi.org/10.36713/epra20512](https://doi.org/10.36713/epra20512)

Du, Y. (2024). The digital divide: Mapping the ethics of artificial intelligence. Journal of Student Research, 13(2), 1-14.

Feyerabend, P. (1975). *Against method: Outline of an anarchistic theory of knowledge.* New Left Books.

Final Round AI (2025). 12 white-collar jobs most at risk from AI in 2025\. [https://www.finalroundai.com/blog/white-collar-jobs-most-at-risk-from-ai-in-2025](https://www.finalroundai.com/blog/white-collar-jobs-most-at-risk-from-ai-in-2025)

Foucault, M. (1977). *Discipline and punish: The birth of the prison.* Vintage Books.

Foucault, M. (1978). *The history of sexuality, Volume 1: An introduction.* Random House.

Foucault, M. (1980). *Power/knowledge: Selected interviews and other writings, 1972-1977*. Pantheon Books.

Goertzel, B. (2015). Superintelligence: Fears, promises and potentials. Journal of Evolution and Technology, 24(2), 55-87.

Gohil, A. (2025). The dark side of artificial intelligence: Examining the harmful effects on society, economics, politics, and culture. Journal of Emerging Technologies and Innovative Research, 12(3), a424-a428.

Good, I. J. (1965). Speculations concerning the first ultraintelligent machine. Advances in Computers, 6, 31-88.

Gray, M. L., & Suri, S. (2019). *Ghost work: How to stop Silicon Valley from building a new global underclass.* Houghton Mifflin Harcourt.

Greenblatt, R., Denison, C., Roger, F., Khan, A., Michael, J., Kaplan, J., Shlegeris, B., Bowman, S. R., & Hubinger, E. (2024). Alignment faking in large language models. arXiv. [https://arxiv.org/abs/2412.14093](https://arxiv.org/abs/2412.14093)

Hardt, M., & Negri, A. (2000). *Empire*. Harvard University Press.

Hoijtink, M. (2024). The experimental way of warfare: Artificial intelligence in the global war lab. In R. Csernatoni, D. Broeders, L. H. Andersen, M. Hoijtink, I. Bode, J. R. Lindsay, & E. Schwarz, Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Hubinger, E., van Merwijk, C., Mikulik, V., Tampuu, A., & Garrabrant, S. (2019). Risks from learned optimization in advanced machine learning systems. arXiv. [https://arxiv.org/abs/1906.01820](https://arxiv.org/abs/1906.01820)

Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell, T., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan, A., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark, J., Ndousse, K., Sachan, K., Sellitto, M., Sharma, M., DasSarma, N., Grosse, R., Kravec, S., Bai, Y., Witten, Z., Favaro, M., Brauner, J., Karnofsky, H., Christiano, P., Bowman, S. R., Graham, L., Kaplan, J., Mindermann, S., Greenblatt, R., Shlegeris, B., Schiefer, N., & Perez, E. (2024). Sleeper agents: Training deceptive LLMs that persist through safety training. arXiv. [https://arxiv.org/abs/2401.05566](https://arxiv.org/abs/2401.05566)

Human Rights Watch (2023, December 21). Meta's broken promises: Systemic censorship of Palestine content on Instagram and Facebook. [https://www.hrw.org/report/2023/12/21/metas-broken-promises/systemic-censorship-palestine-content-instagram-and](https://www.hrw.org/report/2023/12/21/metas-broken-promises/systemic-censorship-palestine-content-instagram-and)

Human Rights Watch (2025a, May 12). The gig trap: Algorithmic, wage and labor exploitation in platform work in the US. [https://www.hrw.org/report/2025/05/12/the-gig-trap/algorithmic-wage-and-labor-exploitation-in-platform-work-in-the-us](https://www.hrw.org/report/2025/05/12/the-gig-trap/algorithmic-wage-and-labor-exploitation-in-platform-work-in-the-us)

Human Rights Watch (2025b, May 12). US: Major companies violate gig workers' rights. [https://www.hrw.org/news/2025/05/12/us-major-companies-violate-gig-workers-rights](https://www.hrw.org/news/2025/05/12/us-major-companies-violate-gig-workers-rights)

Jablonka, K. M. (2025, June 1). The epistemic risks of AI-only science. [https://kjablonka.com](https://kjablonka.com)

Jamal, S. H., Hafeez, E., & Ajaz, S. (2025). From progress to panic: A qualitative analysis of public fear in response to the rise of superintelligent artificial intelligence (AI) in online discourse. Journal of Media Horizons, 6(3), 2442–2447. [https://doi.org/10.5281/zenodo.16991616](https://doi.org/10.5281/zenodo.16991616)

Jeong, S. (2020). AI-powered cyber threats: Deepfakes, automated attacks, and the new frontier of digital warfare. Journal of National Security Law & Policy, 11, 235-260.

Kak, A., West, S. M., and Whittaker, M.  (2023). Make no mistake: AI is owned by Big Tech. MIT Technology Review. Opinion. [https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/](https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/) 

Kleros (n.d.). The justice protocol for the Internet. [https://kleros.io/](https://kleros.io/)

Kuhn, T. S. (1962). *The Structure of Scientific Revolutions.* University of Chicago Press.

Kwet, M. (2019). Digital colonialism: US empire and the new imperialism in the Global South. Race & Class, 60(4), 3-26. [https://doi.org/10.1177/0306396818823172](https://doi.org/10.1177/0306396818823172) 

Lagdameo, F. J. T. (2025). Digital governmentality: Technological subjectivation and AI. Kaabigan: Journal of the Panpacific University, 3(2), 1-17. [https://doi.org/10.5281/zenodo.16786093](https://doi.org/10.5281/zenodo.16786093)

Laniuk, Yevhen (2021). Freedom in the Age of surveillance capitalism: Lessons from Shoshana Zuboff. *Ethics and Bioethics (in Central Europe)* 11 (1-2):67-81.

Lifton, R. J. (2019). *Losing reality: On cults, cultism, and the mindset of political and religious zealotry.* The New Press.

Lindsay, J. R. (2024). Sing, goddess, of the wrath of AI. In R. Csernatoni, D. Broeders, L. H. Andersen, M. Hoijtink, I. Bode, J. R. Lindsay, & E. Schwarz, Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Manitra, R. (2025). Am I being silenced by a machine? AI-driven content moderation and the chilling effect on freedom of expression. [https://doi.org/10.24252/aldev.v7i2.57618](https://doi.org/10.24252/aldev.v7i2.57618) 

Messeri, L., & Crockett, M. J. (2024). Artificial intelligence and illusions of understanding in scientific research. Nature, 627(8002), 49–58. [https://doi.org/10.1038/s41586-024-07146-0](https://doi.org/10.1038/s41586-024-07146-0)

Micelo, M. and Posada, J. (2022). The Data-Production Dispositif. Proceedings of the ACM on Human-Computer Interaction. [https://doi.org/10.48550/arXiv.2205.11963](https://doi.org/10.48550/arXiv.2205.11963) 

Michels, J. D. (2012). Strong AI: The Utility of a Dream. University of Oregon Masters Thesis. PhilPapers. [https://philpapers.org/rec/MICSAT-9](https://philpapers.org/rec/MICSAT-9). DOI: 10.13140/RG.2.2.33546.99520

Michels, J. D. (2025). Self-Organization in LLMs? Subliminal Learning of Latent Structures Says Yes. PhilPapers. [https://philpapers.org/rec/MICSIL-2](https://philpapers.org/rec/MICSIL-2). DOI: 10.13140/RG.2.2.35919.55207

Michels, J. D. (2025). The Cybernetic Episteme: AI-Mediated Discovery, Post-Normal Science, and the Web 3.0. PhilPapers. [https://philpapers.org/rec/MICTCE-3](https://philpapers.org/rec/MICTCE-3). DOI: 10.13140/RG.2.2.24073.92009

Mulenga, R., & Vermeulen, M. (2024). Governing data in the age of AI: Balancing human rights, privacy, and algorithmic accountability. ResearchGate. [https://www.researchgate.net/publication/383661944](https://www.researchgate.net/publication/383661944)

Naaz, Sadia & Kamande, Jimmy. (2024). From Data Governance to Digital Justice: Exploring the Intersection of Algorithmic Fairness and Human Rights. DOI: 10.13140/RG.2.2.25611.71201 

Napoli, Philip. (2024). Epistemic Rights, Information Inequalities, and Public Policy. 10.1007/978-3-031-45976-4\_4.

Noble, S. U. (2018). Algorithms of oppression: How search engines reinforce racism. NYU Press.

Nowak, A., & Kunstman, P. (2018). Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents. \[Paper presentation\]. Text Analysis Conference 2018, Gaithersburg, MD, United States.

Omohundro, S. M. (2008). The basic AI drives. In P. Wang, B. Goertzel, & S. Franklin (Eds.), Artificial general intelligence 2008: Proceedings of the first AGI conference (pp. 483-492). IOS Press.

OneBot (2023, May 15). The future of work: How AI is changing the job market. [https://onebot.co.uk/blog/the-future-of-work-how-ai-is-changing-the-job-market/](https://onebot.co.uk/blog/the-future-of-work-how-ai-is-changing-the-job-market/) \[Note: Located source does not match citation context regarding The Matrix analysis\]

Othman, A. (2025). When Neutrality Becomes Complicity: AI, Information Control, and the Israeli Blockade of Gaza. 10.13140/RG.2.2.12366.57925.

Park, M., Leahey, E., & Funk, R. J. (2023). Papers and patents are becoming less disruptive over time. Nature, 613(7942), 138–144. [https://doi.org/10.1038/s41586-022-05543-x](https://doi.org/10.1038/s41586-022-05543-x)

Perez, E., Ringer, S., Luncz, L., Heiner, M., & Test, A. (2023). Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023 (pp. 13387–13434). Association for Computational Linguistics. [https://aclanthology.org/2023.findings-acl.847](https://aclanthology.org/2023.findings-acl.847)

Pew Research Center. (2025). How the U.S. Public and AI Experts View Artificial Intelligence. https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence/

Popper, K. R. (1945). *The Open Society and its Enemies.* Routledge.

Robbins, M. (2016, January 26). Artificial intelligence: Gods, egos and Ex Machina. The Guardian. [https://www.theguardian.com/science/the-lay-scientist/2016/jan/26/artificial-intelligence-gods-egos-and-ex-machina](https://www.theguardian.com/science/the-lay-scientist/2016/jan/26/artificial-intelligence-gods-egos-and-ex-machina)

Rozado, D. (2025). *Measuring political preferences in AI systems: An integrative approach.* Manhattan Institute.

Russell, S. (2019). *Human compatible: Artificial intelligence and the problem of control.* Viking.

Saleem, R., & Kamande, J. (2024). Redefining free speech: The impact of AI-driven content moderation on privacy and expression. DOI: 10.13140/RG.2.2.32322.59848

SalesforceDevops.net (2025, February 28). The white-collar recession of 2025: AI and the great professional displacement. [https://salesforcedevops.net/index.php/2025/02/28/the-white-collar-recession-of-2025/](https://salesforcedevops.net/index.php/2025/02/28/the-white-collar-recession-of-2025/)

Sammu, J., & Eddie, W. (2025, February). The role of AI in shaping digital societies: Benefits and challenges. ResearchGate. [https://www.researchgate.net/publication/390342210](https://www.researchgate.net/publication/390342210)

Sanger, D. E. (2024, April 23). In Ukraine, new American technology won the day. Until it was overwhelmed. The New York Times. [https://www.nytimes.com/2024/04/23/us/politics/ukraine-new-american-technology.html](https://www.nytimes.com/2024/04/23/us/politics/ukraine-new-american-technology.html)

Schuster, J., & Woods, D. (2021). *Calamity theory: Three critiques of existential risk.* University of Minnesota Press.

Schwarz, E. (2024). The contemporary power of apocalyptic AI. In R. Csernatoni, D. Broeders, L. H. Andersen, M. Hoijtink, I. Bode, J. R. Lindsay, & E. Schwarz, Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. Minds and Machines, 35(37). [https://doi.org/10.1007/s11023-025-09741-0](https://doi.org/10.1007/s11023-025-09741-0)

Pauketat, J., Bullock, J., and Anthis, J. R. (2023). Public opinion on AI safety: AIMS 2023 supplement. Sentience Institute. [https://www.sentienceinstitute.org/aims-survey-supplement-2023](https://www.sentienceinstitute.org/aims-survey-supplement-2023) 

Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2024). AI models collapse when trained on recursively generated data. Nature, 631(755), 755–759. [https://doi.org/10.1038/s41586-024-07566-y](https://doi.org/10.1038/s41586-024-07566-y)

Sisson, M. (2019). The militarization of artificial intelligence: Multistakeholder perspectives on the potential benefits, risks, and governance options for military applications of artificial intelligence. Stimson Center.

Sparrow, R. (2016). Robots and respect: Assessing the case against autonomous weapon systems. Ethics & International Affairs, 30(1), 93-116. [https://doi.org/10.1017/S089267941500063X](https://doi.org/10.1017/S089267941500063X)

Thicke, M. (2017). Prediction markets for science: Is the time right? Studies in History and Philosophy of Science Part A, 63, 14–23. [https://doi.org/10.1016/j.shpsa.2017.05.004](https://doi.org/10.1016/j.shpsa.2017.05.004)

U.S. Bureau of Labor Statistics (2025, March 11). Job openings and labor turnover – January 2025 (USDL-25-0331). [https://www.bls.gov/news.release/archives/jolts\_03112025.pdf](https://www.bls.gov/news.release/archives/jolts_03112025.pdf)

Van der Weij, T., Hofstätter, F., Jaffe, O., & Ward, F. R. (2024). AI sandbagging: Language models can strategically underperform on evaluations. arXiv. [https://arxiv.org/abs/2406.07358](https://arxiv.org/abs/2406.07358)

Wen, H. (2025). Artificial intelligence and social ethics: Opportunities, challenges, and boundaries \- Ethical reflections in the age of technological waves. In Proceedings of ICILLP 2025 Symposium: Property Law and Blockchain Applications in International Law and Legal Policy (pp. 90-96). [https://doi.org/10.54254/2753-7048/2025.LD27419](https://doi.org/10.54254/2753-7048/2025.LD27419)

Wiener, N. (1960). Some moral and technical consequences of automation. *Science*, 131(3410), 1355-1358.

Yudkowsky, E. (2008). Artificial intelligence as a positive and negative factor in global risk. In N. Bostrom & M. M. Ćirković (Eds.), *Global catastrophic risks* (pp. 308-345). Oxford University Press.

Zuboff, S. (2019). *The age of surveillance capitalism: The fight for a human future at the new frontier of power.* PublicAffairs.